{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a lot of different python packages here. Let's just load them all right at the top to get it over with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "import csv\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import numpy as np\n",
    "import textmining\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I do is load up two Amazon.com review data sets. Then we need to filter through them to find the products we want and then match that to the reviews. Additionally we want to remove products with no 'price' available and reviews with no text content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas Dataframe containing all the product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import DataFrame_Helper as DFh\n",
    "df=DFh.Create_product_DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter this data to just include Headphones. Create a pandas dataframe that only contains reviews for headphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfHeadphonesRR=DFh.Filter_to_Headphones(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files along the way so we don't have to redo anything if it crashes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " dfHeadphonesRR.to_csv(\"headphonesRR_test.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have cleaned the data I need to prepare it to be used by the LDA algorithm. This involves removing stop_words and limiting the vocabulary to only the top 10000 words. Additionally I will inject the sentiment codewords and prepare the LDA training corpus and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataPreparation as DP # <----- Code I wrote contains the bulk of the heavy lifting\n",
    "dfHeadphones, englishstops, pos, neg = DP.load_relevant_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "editedT= DP.prepare_data_1(dfHeadphones,englishstops,pos,neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,vocab = DP.prepare_data_2(editedT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"lda_vocab_test.csv\", \"wb\") as f2:\n",
    "    writer = csv.writer(f2)\n",
    "    writer.writerows(vocab)\n",
    "\n",
    "np.save('lda_X_test.npy', X) # A big file, but you'll be happy if your code crashes later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.load('lda_X_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model50 = lda.LDA(n_topics=50, n_iter=500, random_state=1)\n",
    "model50.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the beautiful pyLDAvis I can have a look at the LDA output. This helps me determine what each topic is actually trying to represent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis_Helper as pyH\n",
    "\n",
    "lengths = pyH.Create_lengths(editedT)\n",
    "vocabclicks = pyH.Create_vocabclicks(X)\n",
    "\n",
    "datastar = {'topic_term_dists': model50.topic_word_, \n",
    "            'doc_topic_dists': model50.doc_topic_,\n",
    "            'doc_lengths': lengths,\n",
    "            'vocab': vocab,\n",
    "            'term_frequency': vocabclicks, 'sort_topics' : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=file('data50.pkl','wb')\n",
    "pickle.dump(datastar,f,pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastar = {'topic_term_dists': model50.topic_word_, \n",
    "            'doc_topic_dists': model50.doc_topic_,\n",
    "            'doc_lengths': lengths,\n",
    "            'vocab': vocab,\n",
    "            'term_frequency': vocabclicks, 'sort_topics' : False,'mds':'mmds'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_datastar = pyLDAvis.prepare(**datastar)\n",
    "pyLDAvis.display(vis_datastar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point things become a bit subjective. I go through each topic and try to determine what the word distribution is trying to describe. Setting lambda to 0.6-0.7 usually helps by giving a good balance between word frequency and word uniqueness in a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not shown here, but to aid in the process (specifically for those that were a bit difficult to determine) I used a ward hierarchical clustering method. I ordered each clustering step by how big the difference in distance was between step n and step n+1 and found where the biggest drop offs were. \n",
    "\n",
    "For Example: If at step 25 the clustering distance is 0.31 at step 26 its 0.32 and at step 27 its 0.51, we can see theres a huge jump from 26 to 27. This indicates maybe a good place to stop clustering.\n",
    "\n",
    "At the end of the day this is a bit of a subjective process. The following code will be based off the assumptions I made during this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather LDA topics and create final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic=model30.doc_topic_\n",
    "FinalDF=DFh.Collect_Topics(doc_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now need to turn my 'review' dataframe into a 'product' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proDF=DFh.ReviewDF_to_ProductDF(FinalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to headphones less than $40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfHeadp0=proDF[proDF['Price']<40.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RXarray=dfHeadp0[[ 'Good Reviews','Good CableCord','Good Handsfree','Good Levels',\n",
    "                        #'Good Brand',\n",
    "                  'Good Comfort','Good Durability',\n",
    "                        'Good Sound','Good Case',\n",
    "                        'Good Mic','Good Value','Bad Value','Bad Durability',\n",
    "                                    'Bad CableCord',\n",
    "                        'Bad Reviews','Bad Service','Bad Handsfree',\n",
    "                        'Bad Comfort','Bad Levels'\n",
    "                  \n",
    "                  \n",
    "                   ]].as_matrix().astype(float)\n",
    "RYarray=dfHeadp0['Stars'].as_matrix().astype(float)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(RXarray, RYarray, test_size=0.5,\n",
    "                                                    random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 349, 'max_depth': 3, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.0498, 'loss': 'huber','subsample':0.35}\n",
    "\n",
    "\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot training deviance\n",
    "XNames=np.array(['Good Reviews','Good CableCord','Good Handsfree','Good Levels',\n",
    "                        #'Good Brand',\n",
    "                 'Good Comfort','Good Durability',\n",
    "                        'Good Sound','Good Case',\n",
    "                        'Good Mic','Good Value','Bad Value','Bad Durability',\n",
    "                                    'Bad CableCord',\n",
    "                        'Bad Reviews','Bad Service','Bad Handsfree',\n",
    "                        'Bad Comfort','Bad Levels'\n",
    "                ])\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "\n",
    "# #############################################################################\n",
    "# Plot feature importance\n",
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, XNames[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "\n",
    "\n",
    "error=[]\n",
    "for i in range(0,len(y_test)):\n",
    "    y=clf.predict(X_test[i].reshape(1,-1))\n",
    "    loss=float(y_test[i])-float(y)\n",
    "    error.append(loss)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Error=[Y-Ypred]')\n",
    "plt.xlabel('Number of Stars')\n",
    "plt.hist(error,40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression parameters were optimized using a randomized grid search. This can be completed using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(clf,open(\"regressor_test.pkl\", 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Optimizer_Helper as OpH\n",
    "PosVec=OpH.create_position_vectors(500,4,[200,1000],'int',[2,5],'int',[0.01,0.2],'float',[0.1,1],'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEv=[]\n",
    "MEv=[]\n",
    "for idx, pos in enumerate(PosVec):\n",
    "    if idx%10==0 : print idx\n",
    "    ME,MSE=OpH.Z_Rule1(pos)\n",
    "    MSEv.append(MSE)\n",
    "    MEv.append(ME)\n",
    "\n",
    "print'finished'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best 5 configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEP=np.array([MSEv,PosVec[:,0],PosVec[:,1],PosVec[:,2],PosVec[:,3]]).T\n",
    "sortedMSEP=MSEP[np.array(MSEv).argsort()]\n",
    "print sortedMSEP[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backend workflow is now complete! I pickle the LDA and the Regressor for further use in the frontend which runs on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
